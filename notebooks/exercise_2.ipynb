{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark exercise 2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1,'2013-07-25 00:00:00',11599, 'CLOSED'),\n",
    "    (2,'2013-07-25 00:00:00',256, 'PENDING_PAYMENT'),\n",
    "    (3,'2013-07-25 00:00:00',12111, 'COMPLETE'),\n",
    "    (4,'2013-07-25 00:00:00',8827, 'CLOSED'),\n",
    "    (5,'2013-07-25 00:00:00',11318, 'COMPLETE'),\n",
    "    (6,'2013-07-25 00:00:00',7130, 'COMPLETE'),\n",
    "    (7,'2013-07-25 00:00:00',4530, 'COMPLETE'),\n",
    "    (8,'2013-07-25 00:00:00',2911, 'PROCESSING'),\n",
    "    (9,'2013-07-25 00:00:00',5657, 'PENDING_PAYMENT'),\n",
    "    (10,'2013-07-25 00:00:00',5648, 'PENDING_PAYMENT'),\n",
    "    (11,'2013-07-25 00:00:00',918, 'PAYMENT_REVIEW'),\n",
    "    (12,'2013-07-25 00:00:00',1837, 'CLOSED'),\n",
    "    (13,'2013-07-25 00:00:00',9149, 'PENDING_PAYMENT'),\n",
    "    (14,'2013-07-25 00:00:00',9842, 'PROCESSING'),\n",
    "    (15,'2013-07-25 00:00:00',2568, 'COMPLETE'),\n",
    "    (16,'2013-07-25 00:00:00',7276, 'PENDING_PAYMENT'),\n",
    "    (17,'2013-07-25 00:00:00',2667, 'COMPLETE'),\n",
    "    (18,'2013-07-25 00:00:00',1205, 'CLOSED'),\n",
    "    (19,'2013-07-25 00:00:00',9488, 'PENDING_PAYMENT'),\n",
    "    (20,'2013-07-25 00:00:00',9198, 'PROCESSING')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the RDD to a DF\n",
    "from pyspark.sql import Row\n",
    "df = rdd.map(lambda x: Row(order_id=x[0], order_date=x[1], customer_id=x[2], status=x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "    types.StructField('order_id', types.IntegerType(), True),\n",
    "    types.StructField('order_date', types.StringType(), True),\n",
    "    types.StructField('customer_id', types.IntegerType(), True),\n",
    "    types.StructField('status', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF with Schema\n",
    "df = spark.createDataFrame(df, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn('order_date', F.to_timestamp(df['order_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Schema  \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
