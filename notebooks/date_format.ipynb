{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date Format exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Date Format exercise\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "schema = StructType([\n",
    "    StructField(\"log_level\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"total_occurences\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "# Generate data sample\n",
    "def random_timestamp():\n",
    "    start_date = datetime(2015,1,1,0,0)\n",
    "    random_days = random.randint(0,364)\n",
    "    random_time = timedelta(hours=random.randint(0, 23), minutes=random.randint(0, 59), seconds=random.randint(0, 59))\n",
    "    return start_date + timedelta(days=random_days) + random_time\n",
    "\n",
    "log_levels = [\"INFO\",\"WARN\", \"ERROR\", \"DEBUG\", \"FATAL\"]\n",
    "\n",
    "data = [\n",
    "    (random.choice(log_levels), random_timestamp(), random.randint(10, 50000))\n",
    "    for _ in range(100)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('FATAL', datetime.datetime(2015, 9, 6, 19, 0, 48), 36235), ('ERROR', datetime.datetime(2015, 6, 29, 11, 52, 43), 31182), ('ERROR', datetime.datetime(2015, 3, 29, 23, 43, 6), 3662), ('INFO', datetime.datetime(2015, 10, 21, 13, 32, 37), 36606), ('DEBUG', datetime.datetime(2015, 8, 26, 7, 26, 54), 48107), ('FATAL', datetime.datetime(2015, 8, 10, 1, 32, 18), 20897), ('ERROR', datetime.datetime(2015, 3, 28, 22, 20, 19), 45155), ('DEBUG', datetime.datetime(2015, 7, 23, 21, 16, 12), 46721), ('ERROR', datetime.datetime(2015, 8, 5, 0, 47, 14), 24229), ('FATAL', datetime.datetime(2015, 4, 25, 21, 6, 30), 16353), ('INFO', datetime.datetime(2015, 7, 22, 17, 40, 3), 1110), ('WARN', datetime.datetime(2015, 8, 11, 9, 37, 12), 15852), ('DEBUG', datetime.datetime(2015, 9, 16, 11, 5, 12), 25951), ('ERROR', datetime.datetime(2015, 3, 5, 12, 3, 43), 19500), ('WARN', datetime.datetime(2015, 7, 2, 9, 59), 7524), ('FATAL', datetime.datetime(2015, 10, 23, 10, 24, 15), 41994), ('DEBUG', datetime.datetime(2015, 12, 29, 4, 44, 40), 12542), ('INFO', datetime.datetime(2015, 12, 20, 8, 43, 51), 25397), ('WARN', datetime.datetime(2015, 10, 20, 21, 41, 25), 4418), ('WARN', datetime.datetime(2015, 3, 30, 4, 49, 13), 1714), ('DEBUG', datetime.datetime(2015, 12, 11, 4, 59, 31), 24786), ('DEBUG', datetime.datetime(2015, 1, 11, 23, 17), 21310), ('FATAL', datetime.datetime(2015, 4, 3, 2, 37, 36), 47635), ('ERROR', datetime.datetime(2015, 3, 2, 17, 6, 7), 11827), ('INFO', datetime.datetime(2015, 3, 10, 14, 45, 7), 38280), ('INFO', datetime.datetime(2015, 4, 22, 2, 3, 21), 2183), ('FATAL', datetime.datetime(2015, 6, 14, 22, 29, 36), 22895), ('ERROR', datetime.datetime(2015, 9, 24, 6, 55, 28), 16267), ('FATAL', datetime.datetime(2015, 2, 26, 18, 27, 52), 28831), ('DEBUG', datetime.datetime(2015, 9, 11, 2, 13, 56), 25591), ('DEBUG', datetime.datetime(2015, 11, 16, 3, 27, 9), 40177), ('INFO', datetime.datetime(2015, 7, 23, 15, 7, 39), 34966), ('WARN', datetime.datetime(2015, 1, 1, 22, 48, 20), 33397), ('INFO', datetime.datetime(2015, 2, 19, 13, 44, 37), 48122), ('ERROR', datetime.datetime(2015, 5, 25, 21, 32, 45), 33007), ('WARN', datetime.datetime(2015, 8, 25, 3, 40, 30), 14497), ('DEBUG', datetime.datetime(2015, 6, 13, 8, 48, 45), 26255), ('DEBUG', datetime.datetime(2015, 9, 7, 16, 54, 53), 2789), ('INFO', datetime.datetime(2015, 12, 23, 18, 29, 8), 41802), ('DEBUG', datetime.datetime(2015, 8, 1, 1, 31, 3), 49192), ('INFO', datetime.datetime(2015, 3, 7, 8, 31, 56), 26906), ('INFO', datetime.datetime(2015, 5, 23, 5, 27, 23), 36626), ('ERROR', datetime.datetime(2015, 10, 31, 2, 17, 43), 37207), ('INFO', datetime.datetime(2015, 7, 28, 3, 39, 53), 25436), ('ERROR', datetime.datetime(2015, 11, 19, 10, 58, 37), 6545), ('INFO', datetime.datetime(2015, 10, 12, 0, 29, 8), 48878), ('ERROR', datetime.datetime(2015, 6, 26, 22, 29, 32), 22139), ('FATAL', datetime.datetime(2015, 2, 25, 10, 41, 57), 26956), ('INFO', datetime.datetime(2015, 12, 12, 8, 20, 37), 22395), ('FATAL', datetime.datetime(2015, 9, 13, 22, 28, 53), 28088), ('FATAL', datetime.datetime(2015, 9, 3, 20, 32, 8), 3153), ('DEBUG', datetime.datetime(2015, 11, 26, 20, 4, 31), 20853), ('DEBUG', datetime.datetime(2015, 5, 17, 12, 27, 4), 23943), ('INFO', datetime.datetime(2015, 10, 30, 13, 39, 30), 24922), ('FATAL', datetime.datetime(2015, 7, 30, 17, 59, 33), 30799), ('INFO', datetime.datetime(2015, 11, 12, 22, 11, 55), 5460), ('WARN', datetime.datetime(2015, 7, 5, 17, 14, 32), 48546), ('DEBUG', datetime.datetime(2015, 3, 26, 9, 59, 26), 29532), ('FATAL', datetime.datetime(2015, 9, 19, 21, 39, 1), 17336), ('DEBUG', datetime.datetime(2015, 11, 30, 20, 52, 17), 33065), ('ERROR', datetime.datetime(2015, 2, 1, 18, 59, 39), 9474), ('FATAL', datetime.datetime(2015, 11, 6, 17, 5, 3), 28761), ('WARN', datetime.datetime(2015, 4, 30, 5, 44, 41), 38548), ('WARN', datetime.datetime(2015, 10, 18, 14, 14, 40), 38576), ('ERROR', datetime.datetime(2015, 5, 4, 8, 13, 38), 9641), ('DEBUG', datetime.datetime(2015, 12, 10, 2, 40, 8), 15555), ('FATAL', datetime.datetime(2015, 6, 27, 17, 7, 59), 15780), ('FATAL', datetime.datetime(2015, 10, 12, 13, 39, 37), 13273), ('ERROR', datetime.datetime(2015, 6, 13, 17, 19, 7), 29690), ('INFO', datetime.datetime(2015, 6, 1, 6, 0, 18), 35647), ('WARN', datetime.datetime(2015, 5, 14, 17, 23, 7), 21528), ('ERROR', datetime.datetime(2015, 6, 15, 7, 16, 5), 1366), ('INFO', datetime.datetime(2015, 6, 15, 15, 6, 15), 35129), ('ERROR', datetime.datetime(2015, 3, 11, 20, 51, 32), 44440), ('WARN', datetime.datetime(2015, 8, 23, 6, 59, 29), 4532), ('INFO', datetime.datetime(2015, 5, 3, 18, 57, 53), 41722), ('WARN', datetime.datetime(2015, 12, 25, 23, 59, 10), 30106), ('DEBUG', datetime.datetime(2015, 1, 8, 13, 28, 8), 32111), ('WARN', datetime.datetime(2015, 6, 22, 23, 50, 41), 27087), ('ERROR', datetime.datetime(2015, 2, 23, 5, 49, 18), 30320), ('FATAL', datetime.datetime(2015, 6, 14, 7, 27, 54), 24846), ('DEBUG', datetime.datetime(2015, 1, 13, 6, 25, 29), 37142), ('WARN', datetime.datetime(2015, 5, 17, 0, 46, 36), 19858), ('WARN', datetime.datetime(2015, 8, 6, 6, 28, 35), 33548), ('WARN', datetime.datetime(2015, 9, 15, 21, 29, 19), 46162), ('ERROR', datetime.datetime(2015, 8, 12, 21, 53, 24), 12686), ('ERROR', datetime.datetime(2015, 8, 17, 18, 13, 21), 34567), ('DEBUG', datetime.datetime(2015, 10, 15, 11, 24, 24), 11717), ('WARN', datetime.datetime(2015, 8, 24, 14, 2, 36), 21449), ('ERROR', datetime.datetime(2015, 6, 29, 11, 34, 53), 12391), ('INFO', datetime.datetime(2015, 8, 8, 21, 45, 48), 6117), ('FATAL', datetime.datetime(2015, 11, 3, 15, 57, 1), 17714), ('FATAL', datetime.datetime(2015, 2, 4, 5, 0, 50), 9706), ('DEBUG', datetime.datetime(2015, 8, 12, 10, 45, 42), 31116), ('FATAL', datetime.datetime(2015, 1, 30, 0, 32, 18), 45537), ('ERROR', datetime.datetime(2015, 1, 25, 16, 23), 30175), ('INFO', datetime.datetime(2015, 12, 22, 22, 58, 29), 36960), ('FATAL', datetime.datetime(2015, 6, 12, 9, 13, 3), 26692), ('INFO', datetime.datetime(2015, 4, 10, 19, 20, 39), 27819), ('INFO', datetime.datetime(2015, 2, 6, 7, 33, 26), 6397)]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data,schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------------+\n",
      "|log_level|          timestamp|total_occurences|\n",
      "+---------+-------------------+----------------+\n",
      "|    FATAL|2015-09-06 19:00:48|           36235|\n",
      "|    ERROR|2015-06-29 11:52:43|           31182|\n",
      "|    ERROR|2015-03-29 23:43:06|            3662|\n",
      "|     INFO|2015-10-21 13:32:37|           36606|\n",
      "|    DEBUG|2015-08-26 07:26:54|           48107|\n",
      "+---------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the data sample which we already created\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------------+---------+\n",
      "|log_level|          timestamp|total_occurences|    month|\n",
      "+---------+-------------------+----------------+---------+\n",
      "|    FATAL|2015-09-06 19:00:48|           36235|September|\n",
      "|    ERROR|2015-06-29 11:52:43|           31182|     June|\n",
      "|    ERROR|2015-03-29 23:43:06|            3662|    March|\n",
      "|     INFO|2015-10-21 13:32:37|           36606|  October|\n",
      "|    DEBUG|2015-08-26 07:26:54|           48107|   August|\n",
      "+---------+-------------------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turning timestamp to Date_Format with \"Month\"\n",
    "df1 = df.withColumn(\"month\",\n",
    "              date_format(\"timestamp\", \"MMMM\"))\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------------+---------+-----------------+\n",
      "|log_level|          timestamp|total_occurences|    month|date_of_occurence|\n",
      "+---------+-------------------+----------------+---------+-----------------+\n",
      "|    FATAL|2015-09-06 19:00:48|           36235|September|       2015-09-06|\n",
      "|    ERROR|2015-06-29 11:52:43|           31182|     June|       2015-06-29|\n",
      "|    ERROR|2015-03-29 23:43:06|            3662|    March|       2015-03-29|\n",
      "|     INFO|2015-10-21 13:32:37|           36606|  October|       2015-10-21|\n",
      "|    DEBUG|2015-08-26 07:26:54|           48107|   August|       2015-08-26|\n",
      "+---------+-------------------+----------------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"date_of_occurence\",to_date(df1['timestamp']))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"serverlogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------------+\n",
      "|log_level|          timestamp|total_occurences|\n",
      "+---------+-------------------+----------------+\n",
      "|    FATAL|2015-09-06 19:00:48|           36235|\n",
      "|    ERROR|2015-06-29 11:52:43|           31182|\n",
      "|    ERROR|2015-03-29 23:43:06|            3662|\n",
      "|     INFO|2015-10-21 13:32:37|           36606|\n",
      "|    DEBUG|2015-08-26 07:26:54|           48107|\n",
      "|    FATAL|2015-08-10 01:32:18|           20897|\n",
      "|    ERROR|2015-03-28 22:20:19|           45155|\n",
      "|    DEBUG|2015-07-23 21:16:12|           46721|\n",
      "|    ERROR|2015-08-05 00:47:14|           24229|\n",
      "|    FATAL|2015-04-25 21:06:30|           16353|\n",
      "+---------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check serverlogs dataset in SQL\n",
    "spark.sql(\"\"\"\n",
    "    select * from serverlogs limit 10\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|log_level|month_name|total_count|\n",
      "+---------+----------+-----------+\n",
      "|    FATAL|     April|          2|\n",
      "|     INFO|     April|          2|\n",
      "|     WARN|     April|          1|\n",
      "|    FATAL|    August|          1|\n",
      "|    ERROR|    August|          3|\n",
      "|    DEBUG|    August|          3|\n",
      "|     WARN|    August|          5|\n",
      "|     INFO|    August|          1|\n",
      "|    DEBUG|  December|          3|\n",
      "|     INFO|  December|          4|\n",
      "|     WARN|  December|          1|\n",
      "|     INFO|  February|          2|\n",
      "|    FATAL|  February|          3|\n",
      "|    ERROR|  February|          2|\n",
      "|     WARN|   January|          1|\n",
      "|    DEBUG|   January|          3|\n",
      "|    FATAL|   January|          1|\n",
      "|    ERROR|   January|          1|\n",
      "|    DEBUG|      July|          1|\n",
      "|     INFO|      July|          3|\n",
      "+---------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update data type by SQL query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT log_level,\n",
    "          date_format(timestamp, 'MMMM') as month_name,\n",
    "          count(*) as total_count\n",
    "    FROM serverlogs\n",
    "    GROUP BY log_level, month_name\n",
    "    ORDER BY month_name\n",
    "          \"\"\").show()\n",
    "\n",
    "#Note: if the timestamp is 'StringType': \n",
    "# -> date_format(to_date(timestamp, 'yyyy-MM-dd  HH:mm:ssss'), 'MMMM') as month_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But the query above didn't return by month( it returns by alphabet)\n",
    "# Let's make an update!\n",
    "df_generate = spark.sql(\"\"\"\n",
    "    SELECT log_level,\n",
    "          date_format(timestamp, 'MMMM') as month_name,\n",
    "          int(date_format(timestamp, 'M')) as month_num,\n",
    "          count(*) as total_count\n",
    "    FROM serverlogs\n",
    "    GROUP BY log_level, month_name, month_num\n",
    "    ORDER BY month_num\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_generate.drop(\"month_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|log_level|month_name|total_count|\n",
      "+---------+----------+-----------+\n",
      "|    DEBUG|   January|          3|\n",
      "|     WARN|   January|          1|\n",
      "|    ERROR|   January|          1|\n",
      "|    FATAL|   January|          1|\n",
      "|     INFO|  February|          2|\n",
      "|    FATAL|  February|          3|\n",
      "|    ERROR|  February|          2|\n",
      "|    ERROR|     March|          5|\n",
      "|     WARN|     March|          1|\n",
      "|     INFO|     March|          2|\n",
      "+---------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean unnecessary column with month_num\n",
    "# Because it already make the date column in ascending order\n",
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+------+-----+-----+-----+-----+------+-----+------+-----+------+\n",
      "|log_level|   01|   02|    03|   04|   05|   06|   07|    08|   09|    10|   11|    12|\n",
      "+---------+-----+-----+------+-----+-----+-----+-----+------+-----+------+-----+------+\n",
      "|     INFO| NULL|54519| 65186|30002|78348|70776|61512|  6117| NULL|110406| 5460|126554|\n",
      "|    ERROR|30175|39794|124584| NULL|42648|96768| NULL| 71482|16267| 37207| 6545|  NULL|\n",
      "|     WARN|33397| NULL|  1714|38548|41386|27087|56070| 89878|46162| 42994| NULL| 30106|\n",
      "|    DEBUG|90563| NULL| 29532| NULL|23943|26255|46721|128415|54331| 11717|94095| 52883|\n",
      "|    FATAL|45537|65493|  NULL|63988| NULL|90213|30799| 20897|84812| 55267|46475|  NULL|\n",
      "+---------+-----+-----+------+-----+-----+-----+-----+------+-----+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot\n",
    "df = df.withColumn(\"total_occurences\", col(\"total_occurences\").cast(\"int\"))\n",
    "\n",
    "df.createOrReplaceTempView(\"serverlogs\")\n",
    "\n",
    "df_pi = spark.sql(\"\"\"\n",
    "    SELECT log_level,\n",
    "          date_format(timestamp,\"MM\") as month_name,\n",
    "            total_occurences\n",
    "    FROM serverlogs\n",
    "          \"\"\").groupBy(\"log_level\").pivot(\"month_name\").sum(\"total_occurences\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the list name\n",
    "month_list = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+------+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|log_level|January|February| March|April|  May| June| July|August|September|October|November|December|\n",
      "+---------+-------+--------+------+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|     INFO|      0|   54519| 65186|30002|78348|70776|61512|  6117|        0| 110406|    5460|  126554|\n",
      "|    ERROR|  30175|   39794|124584|    0|42648|96768|    0| 71482|    16267|  37207|    6545|       0|\n",
      "|     WARN|  33397|       0|  1714|38548|41386|27087|56070| 89878|    46162|  42994|       0|   30106|\n",
      "|    DEBUG|  90563|       0| 29532|    0|23943|26255|46721|128415|    54331|  11717|   94095|   52883|\n",
      "|    FATAL|  45537|   65493|     0|63988|    0|90213|30799| 20897|    84812|  55267|   46475|       0|\n",
      "+---------+-------+--------+------+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert the month_list into pivot name appearance\n",
    "df = df.withColumn(\"total_occurences\", col(\"total_occurences\").cast(\"int\"))\n",
    "\n",
    "df.createOrReplaceTempView(\"serverlogs\")\n",
    "\n",
    "# update the list name\n",
    "month_list = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "df_pi = spark.sql(\"\"\"\n",
    "    SELECT log_level,\n",
    "          date_format(timestamp,\"MMMM\") as month_name,\n",
    "            total_occurences\n",
    "    FROM serverlogs\n",
    "          \"\"\").groupBy(\"log_level\").pivot(\"month_name\",month_list).sum(\"total_occurences\").fillna(0).show()\n",
    "\n",
    "# Note: using fillna(0) instead of isnull()\n",
    "# Because ISNULL() only when applying conditional logic (e.g: filtering rows where NULL exists)\n",
    "# withColumn(\"January\", when(col(\"January\").isNull(),0).otherwise(col(\"January\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------------+\n",
      "|log_level|          timestamp|total_occurences|\n",
      "+---------+-------------------+----------------+\n",
      "|    FATAL|2015-09-06 19:00:48|           36235|\n",
      "|    ERROR|2015-06-29 11:52:43|           31182|\n",
      "|    ERROR|2015-03-29 23:43:06|            3662|\n",
      "|     INFO|2015-10-21 13:32:37|           36606|\n",
      "|    DEBUG|2015-08-26 07:26:54|           48107|\n",
      "|    FATAL|2015-08-10 01:32:18|           20897|\n",
      "|    ERROR|2015-03-28 22:20:19|           45155|\n",
      "|    DEBUG|2015-07-23 21:16:12|           46721|\n",
      "|    ERROR|2015-08-05 00:47:14|           24229|\n",
      "|    FATAL|2015-04-25 21:06:30|           16353|\n",
      "+---------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * from serverlogs limit 10\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
