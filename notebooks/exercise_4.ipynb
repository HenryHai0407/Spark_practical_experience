{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4: DataFrame exercise (Simple and Grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001E9324C7190>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL exercise 4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([\n",
    "    (536378, \"NULL\", \"PACK OF 60 DINOSAUR CAKE CASES\", 24, \"01-12-2010\", 0.55, 17850, \"United Kingdom\"),\n",
    "    (536378, \"NULL\", \"PACK OF 72 SKULL CAKE CASES\", 24, \"01-12-2010\", 0.55, 17850, \"United States\"),\n",
    "    (536378, \"84991A\", \"72 SWEETHEART FAIRY CAKE CASES\", 120, \"01-12-2010\", 0.55, 17850, \"United Kingdom\"),\n",
    "    (536378, \"84992B\", \"72 RETROSPOT TEA SET CERAMIC HEART\", 120, \"01-12-2010\", 0.55, 17850, \"United Kingdom\"),\n",
    "    (536378, \"84993C\", \"60 TEATIME FAIRY CAKE CASES\", 120, \"01-12-2010\", 0.55, 17850, \"United Kingdom\"),\n",
    "    (536378, \"84994\", \"60 CAKE CASES VINTAGE CHRISTMAS\", 120, \"01-12-2010\", 0.55, 17850, \"United States\"),\n",
    "    (536381, \"22727\", \"ALARM CLOCK BAKELIKE PINK\", 24, \"01-12-2010\", 3.75, 15311, \"United Kingdom\"),\n",
    "    (536381, \"22726\", \"ALARM CLOCK BAKELIKE RED\", 24, \"01-12-2010\", 3.75, 15311, \"Germany\"),\n",
    "    (536381, \"22730\", \"ALARM CLOCK BAKELIKE IVORY\", 24, \"01-12-2010\", 3.75, 15311, \"Germany\"),\n",
    "    (536381, \"22367\", \"CHILDRENS APRON SPACEBOY DESIGN\", 8, \"01-12-2010\", 1.95, 15311, \"United Kingdom\"),\n",
    "    (536381, \"22629\", \"SPACEBOY LUNCH BOX\", 12, \"01-12-2010\", 1.95, 15311, \"Austria\"),\n",
    "    (536381, \"22659\", \"LUNCH BOX I LOVE LONDON\", 12, \"01-12-2010\", 1.95, 15311, \"United Kingdom\"),\n",
    "    (536381, \"22631\", \"CIRCUS PARADE LUNCH BOX\", 12, \"01-12-2010\", 1.95, 15311, \"Switzerland\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df = rdd.map(lambda x: Row(InvoiceNo=x[0], StockCode=x[1], Description=x[2], Quantity=x[3], InvoiceDate=x[4], UnitPrice=x[5], CustomerID=x[6], Country=x[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create its Schema\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(df, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536378|     NULL|PACK OF 60 DINOSA...|      24| 01-12-2010|     0.55|     17850|United Kingdom|\n",
      "|   536378|     NULL|PACK OF 72 SKULL ...|      24| 01-12-2010|     0.55|     17850| United States|\n",
      "|   536378|   84991A|72 SWEETHEART FAI...|     120| 01-12-2010|     0.55|     17850|United Kingdom|\n",
      "|   536378|   84992B|72 RETROSPOT TEA ...|     120| 01-12-2010|     0.55|     17850|United Kingdom|\n",
      "|   536378|   84993C|60 TEATIME FAIRY ...|     120| 01-12-2010|     0.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536378|     NULL|PACK OF 60 DINOSA...|      24| 2010-12-01|     0.55|     17850|United Kingdom|\n",
      "|   536378|     NULL|PACK OF 72 SKULL ...|      24| 2010-12-01|     0.55|     17850| United States|\n",
      "|   536378|   84991A|72 SWEETHEART FAI...|     120| 2010-12-01|     0.55|     17850|United Kingdom|\n",
      "|   536378|   84992B|72 RETROSPOT TEA ...|     120| 2010-12-01|     0.55|     17850|United Kingdom|\n",
      "|   536378|   84993C|60 TEATIME FAIRY ...|     120| 2010-12-01|     0.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update InvoiceDate column to DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "df1 = df.withColumn(\"InvoiceDate\", to_date(df[\"InvoiceDate\"], \"dd-MM-yyyy\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1: count total rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2: total number of invoices\n",
    "df.select(\"InvoiceNo\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------+---------+\n",
      "|TotalRows|TotalInvoices|Total_Quantity|Avg_Price|\n",
      "+---------+-------------+--------------+---------+\n",
      "|       13|            2|           644|      1.7|\n",
      "+---------+-------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1,2a\n",
    "from pyspark.sql.functions import *\n",
    "df.select(count(\"*\").alias(\"TotalRows\"), countDistinct(\"InvoiceNo\").alias(\"TotalInvoices\"),\n",
    "          sum(\"Quantity\").alias(\"Total_Quantity\"),\n",
    "          round(avg(\"UnitPrice\"),1).alias(\"Avg_Price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|count_rows|count_distinct|\n",
      "+----------+--------------+\n",
      "|        13|             2|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1,2b\n",
    "df.selectExpr(\"count(*) as count_rows\",\n",
    "              \"count(distinct (InvoiceNo)) as count_distinct\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|          644|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3: total number of quantity sold\n",
    "from pyspark.sql.functions import *\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   avg(UnitPrice)|\n",
      "+-----------------+\n",
      "|1.719230789404649|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4: average unit price\n",
    "df.select(avg(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|TotalQuantity| AverageUnitPrice|\n",
      "+-------------+-----------------+\n",
      "|          644|1.719230789404649|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4: second way\n",
    "df.createOrReplaceTempView(\"sales\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT sum(Quantity) as TotalQuantity, avg(UnitPrice) as AverageUnitPrice\n",
    "FROM sales\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|  sum(TotalPrice)|\n",
      "+-----------------+\n",
      "|646.2000064849854|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4: third way\n",
    "df.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\")).select(sum(\"TotalPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|       Country|sum(Quantity)|\n",
      "+--------------+-------------+\n",
      "|United Kingdom|          428|\n",
      "| United States|          144|\n",
      "|       Germany|           48|\n",
      "|       Austria|           12|\n",
      "|   Switzerland|           12|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: Total quantity sold by each country\n",
    "df1 = df.groupBy(\"Country\").sum(\"Quantity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|       Country|TotalQuantity|\n",
      "+--------------+-------------+\n",
      "|United Kingdom|          428|\n",
      "| United States|          144|\n",
      "|       Germany|           48|\n",
      "|       Austria|           12|\n",
      "|   Switzerland|           12|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: Second way\n",
    "df1 = df.groupBy(\"Country\").agg(sum(\"Quantity\").alias(\"TotalQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|       Country|TotalQuantity|\n",
      "+--------------+-------------+\n",
      "|United Kingdom|          428|\n",
      "| United States|          144|\n",
      "|       Germany|           48|\n",
      "|       Austria|           12|\n",
      "|   Switzerland|           12|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: Third way\n",
    "spark.sql(\"\"\"\n",
    "SELECT Country, sum(Quantity) as TotalQuantity\n",
    "FROM sales\n",
    "GROUP BY Country\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+\n",
      "|       Country|total_quant|total_amount|\n",
      "+--------------+-----------+------------+\n",
      "|United Kingdom|        428|       340.2|\n",
      "| United States|        144|        79.2|\n",
      "|       Germany|         48|       180.0|\n",
      "|       Austria|         12|        23.4|\n",
      "|   Switzerland|         12|        23.4|\n",
      "+--------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: Fourth way (like Second way but using ALIAS() also)\n",
    "df.groupBy(\"Country\") \\\n",
    ".agg(sum(\"Quantity\").alias(\"total_quant\"),\n",
    "     round(sum(df.Quantity * df.UnitPrice),1).alias(\"total_amount\")) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+\n",
      "|       Country|total_quantity|total_amount1|\n",
      "+--------------+--------------+-------------+\n",
      "|United Kingdom|           428|        340.2|\n",
      "| United States|           144|         79.2|\n",
      "|       Germany|            48|        180.0|\n",
      "|       Austria|            12|         23.4|\n",
      "|   Switzerland|            12|         23.4|\n",
      "+--------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "#1: Fifth way\n",
    "\n",
    "df.groupBy(\"Country\") \\\n",
    "    .agg(expr(\"sum(Quantity) as total_quantity\"),\n",
    "         expr(\"round(sum(Quantity * UnitPrice),2) as total_amount1\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|       Country|   sum(TotalPrice)|\n",
      "+--------------+------------------+\n",
      "|United Kingdom| 340.2000026702881|\n",
      "| United States| 79.20000076293945|\n",
      "|       Germany|             180.0|\n",
      "|       Austria|23.400001525878906|\n",
      "|   Switzerland|23.400001525878906|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2: Total price sold by each country\n",
    "df1=df.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\")).groupBy(\"Country\").sum(\"TotalPrice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|       Country|   sum(TotalPrice)|\n",
      "+--------------+------------------+\n",
      "|United Kingdom| 340.2000026702881|\n",
      "| United States| 79.20000076293945|\n",
      "|       Germany|             180.0|\n",
      "|       Austria|23.400001525878906|\n",
      "|   Switzerland|23.400001525878906|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2: Second way\n",
    "df.select(col(\"Country\"), (col(\"Quantity\") * col(\"UnitPrice\")).alias(\"TotalPrice\")).groupBy(\"Country\").sum(\"TotalPrice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|       Country|        TotalPrice|\n",
      "+--------------+------------------+\n",
      "|United Kingdom| 340.2000026702881|\n",
      "| United States| 79.20000076293945|\n",
      "|       Germany|             180.0|\n",
      "|       Austria|23.400001525878906|\n",
      "|   Switzerland|23.400001525878906|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3: Third way\n",
    "spark.sql(\"\"\"\n",
    "SELECT Country, sum(Quantity * UnitPrice) as TotalPrice\n",
    "FROM sales\n",
    "GROUP BY Country\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
